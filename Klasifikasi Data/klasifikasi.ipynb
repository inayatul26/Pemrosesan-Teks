{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**EKSPERIMEN 1**\n",
        "\n",
        "Menggunakan algoritma naive bayes dan ekstraksi fitur TF-IDF"
      ],
      "metadata": {
        "id": "xQkJUQeRKJub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/250_Data_Pelabelan_Manual.csv\")\n",
        "\n",
        "X = df['review_cleaned']\n",
        "y = df['label']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = nb.predict(X_test_tfidf)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Akurasi Model: \", acc)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONnaMiGWKPuC",
        "outputId": "afedeae8-17f3-4d5f-f394-b4383271591d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Model:  0.84\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.95      0.95      0.95        20\n",
            "      netral       0.50      0.33      0.40         6\n",
            "     positif       0.81      0.88      0.84        24\n",
            "\n",
            "    accuracy                           0.84        50\n",
            "   macro avg       0.75      0.72      0.73        50\n",
            "weighted avg       0.83      0.84      0.83        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EKSPERIMEN 2**\n",
        "\n",
        "Menggunakan algoritma SVM dan ektraksi fitur TF-IDF\n"
      ],
      "metadata": {
        "id": "CES2FbN_Lxf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/250_Data_Pelabelan_Manual.csv\")\n",
        "\n",
        "X = df['review_cleaned']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "svm_model = LinearSVC()\n",
        "svm_model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(\"Akurasi Model SVM:\", acc)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ3p5ANaLw7p",
        "outputId": "34ff3e0e-1244-4813-bd19-ddc75f585802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi Model SVM: 0.76\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.81      0.85      0.83        20\n",
            "      netral       0.30      0.50      0.38         6\n",
            "     positif       0.95      0.75      0.84        24\n",
            "\n",
            "    accuracy                           0.76        50\n",
            "   macro avg       0.69      0.70      0.68        50\n",
            "weighted avg       0.81      0.76      0.78        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EKSPERIMEN 3**\n",
        "\n",
        "Menggunakan Algoritma LSTM"
      ],
      "metadata": {
        "id": "mowhiFenPniX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/250_Data_Pelabelan_Manual.csv\")\n",
        "\n",
        "X = df['review_cleaned']\n",
        "y = df['label']\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "y_encoded = to_categorical(y_encoded)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "max_words = 5000\n",
        "max_len = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, split=' ')\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 128, input_length=max_len))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(y_encoded.shape[1], activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_pad,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    batch_size=8,\n",
        "    validation_data=(X_test_pad, y_test)\n",
        ")\n",
        "\n",
        "loss, acc = model.evaluate(X_test_pad, y_test)\n",
        "print(f\"Akurasi Model LSTM: {acc:.2f}\")\n",
        "\n",
        "y_test_arg = np.argmax(y_test, axis=1)\n",
        "y_pred = np.argmax(model.predict(X_test_pad), axis=1)\n",
        "\n",
        "print(classification_report(y_test_arg, y_pred, target_names=label_encoder.classes_))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuUT-5x0NTBI",
        "outputId": "412241f3-0c72-4c65-b23f-13150274ef12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 267ms/step - accuracy: 0.3441 - loss: 1.0929 - val_accuracy: 0.5200 - val_loss: 1.0051\n",
            "Epoch 2/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.4900 - loss: 1.0490 - val_accuracy: 0.4200 - val_loss: 0.9645\n",
            "Epoch 3/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 176ms/step - accuracy: 0.7176 - loss: 0.8986 - val_accuracy: 0.7400 - val_loss: 0.6267\n",
            "Epoch 4/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 255ms/step - accuracy: 0.8091 - loss: 0.5340 - val_accuracy: 0.8000 - val_loss: 0.5411\n",
            "Epoch 5/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 174ms/step - accuracy: 0.9149 - loss: 0.2136 - val_accuracy: 0.8200 - val_loss: 0.5340\n",
            "Epoch 6/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 179ms/step - accuracy: 0.9870 - loss: 0.0803 - val_accuracy: 0.7000 - val_loss: 0.7491\n",
            "Epoch 7/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 246ms/step - accuracy: 1.0000 - loss: 0.0341 - val_accuracy: 0.7800 - val_loss: 0.6800\n",
            "Epoch 8/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 174ms/step - accuracy: 1.0000 - loss: 0.0082 - val_accuracy: 0.7200 - val_loss: 0.7546\n",
            "Epoch 9/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 230ms/step - accuracy: 0.9946 - loss: 0.0174 - val_accuracy: 0.7600 - val_loss: 0.6751\n",
            "Epoch 10/10\n",
            "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 167ms/step - accuracy: 0.9924 - loss: 0.0347 - val_accuracy: 0.7800 - val_loss: 0.5972\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7596 - loss: 0.6466\n",
            "Akurasi Model LSTM: 0.78\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f70cc288680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/2\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 861ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f70cc288680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 417ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.76      0.95      0.84        20\n",
            "      netral       0.50      0.50      0.50         6\n",
            "     positif       0.89      0.71      0.79        24\n",
            "\n",
            "    accuracy                           0.78        50\n",
            "   macro avg       0.72      0.72      0.71        50\n",
            "weighted avg       0.79      0.78      0.78        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EKSPERIMEN 4**\n",
        "\n",
        "Menggunakan algoritma naive bayes dan ekstraksi fitur word2vec"
      ],
      "metadata": {
        "id": "z9QXBGS6RFnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5p6bAX8Q8X-",
        "outputId": "0badce57-b271-4dd1-dae5-885de8935526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from gensim.models import Word2Vec\n",
        "from tqdm import tqdm\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/250_Data_Pelabelan_Manual.csv\")\n",
        "\n",
        "X = df['review_cleaned']\n",
        "y = df['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_sentences = [row.split() for row in X_train]\n",
        "test_sentences  = [row.split() for row in X_test]\n",
        "\n",
        "w2v_model = Word2Vec(\n",
        "    sentences=train_sentences,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "def get_sentence_vector(words, model, vector_size=100):\n",
        "    word_vecs = np.zeros((vector_size,))\n",
        "    count = 0\n",
        "    for word in words:\n",
        "        if word in model.wv:\n",
        "            word_vecs += model.wv[word]\n",
        "            count += 1\n",
        "    if count > 0:\n",
        "        word_vecs /= count\n",
        "    return word_vecs\n",
        "\n",
        "X_train_vec = np.array([get_sentence_vector(words, w2v_model) for words in train_sentences])\n",
        "X_test_vec  = np.array([get_sentence_vector(words, w2v_model) for words in test_sentences])\n",
        "\n",
        "nb = GaussianNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "\n",
        "y_pred = nb.predict(X_test_vec)\n",
        "\n",
        "print(\"Akurasi:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUJaGA7kQoYF",
        "outputId": "6f72739f-942f-4133-fe88-cd96885c83c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Akurasi: 0.74\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.88      0.70      0.78        20\n",
            "      netral       0.38      0.50      0.43         6\n",
            "     positif       0.77      0.83      0.80        24\n",
            "\n",
            "    accuracy                           0.74        50\n",
            "   macro avg       0.67      0.68      0.67        50\n",
            "weighted avg       0.76      0.74      0.75        50\n",
            "\n"
          ]
        }
      ]
    }
  ]
}